<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Uni-CoT: Towards Unified Chain-of-Thought Reasoning Across Text and Vision</title>

<meta name="description" content="Uni-CoT(UniCoT) is a unified framework that enables coherent and scalable multimodal reasoning over dynamic visual states via a novel two-level reasoning architecture and an efficient training paradigm.">
<meta name="keywords" content="computer-vision,deep-learning,artificial-intelligence,cot,multimodal,unified-model,any-to-any,llm,vision-language-pretraining,chain-of-thought,chain-of-thought-reasoning,uni-cot,unicot">

<link rel="canonical" href="https://sais-fuxi.github.io/projects/uni-cot/">
<meta name="robots" content="index,follow,max-image-preview:large">

<!-- Open Graph -->
<meta property="og:type" content="article">
<meta property="og:title" content="Uni-CoT: Towards Unified Chain-of-Thought Reasoning Across Text and Vision">
<meta property="og:description" content="Uni-CoT(UniCoT) is a unified framework that enables coherent and scalable multimodal reasoning over dynamic visual states via a novel two-level reasoning architecture and an efficient training paradigm.">
<meta property="og:url" content="https://sais-fuxi.github.io/projects/uni-cot/">
<meta property="og:image" content="https://sais-fuxi.github.io/projects/uni-cot/files/og-uni-cot-1200x630.png">


<link href="./files/style.css" rel="stylesheet">
<script type="text/javascript" src="./files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./files/jquery.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<meta name="google-site-verification" content="fhj09Oi6LV8is0pK6zlzfKa0UsyeXQu3LjVv5kFsc5c" />

<!-- JSON-LD -->
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@graph": [
    {
      "@type": "Organization",
      "@id": "https://github.com/SAIS-FUXI",
      "name": "SAIS-FUXI",
      "url": "https://github.com/SAIS-FUXI"
    },
    {
      "@type": ["ScholarlyArticle", "Article"],
      "@id": "https://sais-fuxi.github.io/projects/uni-cot/",
      "mainEntityOfPage": { 
        "@type": "WebPage", 
        "@id": "https://sais-fuxi.github.io/projects/uni-cot/" 
      },
      "headline": "Uni-CoT: Towards Unified Chain-of-Thought Reasoning Across Text and Vision",
      "keywords": "computer-vision,deep-learning,artificial-intelligence,cot,multimodal,unified-model,any-to-any,llm,vision-language-pretraining,chain-of-thought,chain-of-thought-reasoning,uni-cot,unicot",
      "author": [
        {
          "@type": "Person",
          "name": "Luozheng Qin",
          "sameAs": "https://scholar.google.com/citations?user=41BWCzkAAAAJ",
          "affiliation": {
            "@type": "Organization",
            "name": "ShangHai Academy of AI for Science"
          }
        },
        {
          "@type": "Person",
          "name": "Jia Gong",
          "sameAs": "https://scholar.google.com/citations?user=ZV-ThegAAAAJ",
          "affiliation": {
            "@type": "Organization",
            "name": "ShangHai Academy of AI for Science"
          }
        },
        {
          "@type": "Person",
          "name": "Yuqing Sun",
          "sameAs": "https://github.com/sunyuqingannie",
          "affiliation": {
            "@type": "Organization",
            "name": "ShangHai Academy of AI for Science"
          }
        },
        {
          "@type": "Person",
          "name": "Tianjiao Li",
          "sameAs": "https://scholar.google.com/citations?user=so6xMg8AAAAJ",
          "affiliation": {
            "@type": "Organization",
            "name": "Nanyang Technological University"
          }
        },
        {
          "@type": "Person",
          "name": "Mengping Yang",
          "sameAs": "https://scholar.google.com/citations?user=yF34LtcAAAAJ",
          "affiliation": {
            "@type": "Organization",
            "name": "ShangHai Academy of AI for Science"
          }
        },
        {
          "@type": "Person",
          "name": "Xiaomeng Yang",
          "sameAs": "https://scholar.google.com/citations?user=7evPWQYAAAAJ",
          "affiliation": {
            "@type": "Organization",
            "name": "ShangHai Academy of AI for Science"
          }
        },
        {
          "@type": "Person",
          "name": "Chao Qu",
          "sameAs": "https://scholar.google.com/citations?user=DI2NyPsAAAAJ",
          "affiliation": {
            "@type": "Organization",
            "name": "INFTech"
          }
        },
        {
          "@type": "Person",
          "name": "Zhiyu Tan",
          "sameAs": "https://github.com/SAIS-FUXI",
          "affiliation": [
            { "@type": "Organization", "name": "ShangHai Academy of AI for Science" },
            { "@type": "Organization", "name": "Fudan University" }
          ]
        },
        {
          "@type": "Person",
          "name": "Hao Li",
          "sameAs": "https://scholar.google.com/citations?user=pHN-QIwAAAAJ",
          "affiliation": [
            { "@type": "Organization", "name": "ShangHai Academy of AI for Science" },
            { "@type": "Organization", "name": "Fudan University" }
          ]
        }
      ],
      "dateModified": "2025-07-29T19:34:36+08:00",
      "datePublished": "2025-07-29T01:53:30+08:00",
      "inLanguage": "en",
      "url": "https://sais-fuxi.github.io/projects/uni-cot/",
      "image": {
        "@type": "ImageObject",
        "url": "https://sais-fuxi.github.io/projects/uni-cot/files/og-uni-cot-1200x630.png",
        "width": 1200,
        "height": 633
      },
      "description": "Uni-CoT(UniCoT) is a unified framework that enables coherent and scalable multimodal reasoning over dynamic visual states via a novel two-level reasoning architecture and an efficient training paradigm."
    },
    {
      "@type": "SoftwareSourceCode",
      "name": "Uni-CoT",
      "codeRepository": "https://github.com/Fr0zenCrane/UniCoT",
      "programmingLanguage": "Python",
      "url": "https://github.com/Fr0zenCrane/UniCoT",
      "license": "https://opensource.org/licenses/Apache-2.0"
    }
  ]
}
</script>

</head>
<style> .centered {text-align: center;}</style>


<body>
<div class="content">
  <figure style="text-align: center;">
    <img src="./files/logo.png" alt="Uni-CoT Logo" class="teaser-gif" style="width:40%;">
  </figure>
  <h1><strong>Uni-CoT: Towards Unified Chain-of-Thought Reasoning Across Text and Vision</strong></h1>
  <p id="authors" class="serif">
    <span style="text-align: justify; font-size: 0.9em">
      <a href="https://scholar.google.com/citations?user=41BWCzkAAAAJ&hl=zh-CN&oi=ao">Luozheng Qin</a><sup>1</sup><sup>*</sup>,
      <a href="https://scholar.google.com/citations?user=ZV-ThegAAAAJ&hl=zh-CN&oi=ao">Jia Gong</a><sup>1</sup><sup>*</sup>,
      <a href="https://github.com/sunyuqingannie">Yuqing Sun</a><sup>1</sup><sup>*</sup>,
      <a href="https://scholar.google.com/citations?hl=zh-CN&user=so6xMg8AAAAJ">Tianjiao Li</a><sup>3</sup>,
      <br>
      <a href="https://scholar.google.com/citations?user=yF34LtcAAAAJ&hl=zh-CN&oi=ao">Mengping Yang</a><sup>1</sup>,
      <a href="https://scholar.google.com/citations?hl=zh-CN&user=7evPWQYAAAAJ">Xiaomeng Yang</a><sup>1</sup>,
      <a href="https://scholar.google.com/citations?hl=en&user=DI2NyPsAAAAJ&view_op=list_works&sortby=pubdate">Chao Qu</a><sup>4</sup>,
      <a href="https://github.com/SAIS-FUXI">Zhiyu Tan</a><sup>1,2</sup><sup>#&dagger;</sup>,
      <a href="https://scholar.google.com/citations?user=pHN-QIwAAAAJ&hl=zh-CN">Hao Li</a><sup>1,2</sup><sup>&dagger;</sup>
    </span>
    <br>
  <span style="font-size: 1.0em; margin-top: 0.6em">
    <a><sup>1</sup>ShangHai Academy of AI for Science</a>
    <a><sup>2</sup>Fudan University</a>
    <a><sup>3</sup>Nanyang Technological University</a>
    <a><sup>4</sup>INFTech</a>
  <br>
  </span>
<span style="font-size: 12pt;"><b><sup>*</sup>Equal contribution</b><b> <sup>#</sup>Project leader</b><b> <sup>&dagger;</sup>Corresponding author</b></span>
  </p>
    <font size="+2">
          <p style="text-align: center;">
      <a href="https://github.com/Fr0zenCrane/UniCoT" target="_blank">[Code]</a> &nbsp;&nbsp;&nbsp;&nbsp;
      <a href="https://github.com/Fr0zenCrane/UniCoT/blob/main/docs/technical_report.md" target="_blank">[Report]</a> &nbsp;&nbsp;&nbsp;&nbsp;
      <a href="https://huggingface.co/Fr0zencr4nE/UniCoT-7B-MoT" target="_blank">[Model]</a> &nbsp;&nbsp;&nbsp;&nbsp;
      <a href="files/bibtex.txt" target="_blank">[BibTeX]</a>
          </p>
    </font>
</div>

<div class="content">
  <h2 style="text-align:center;">&#x1F525;News</h2>
  <p>&#x2705; 2025.07.29 We released UniCoT-7B-MoT, which extends Bagel-7B-MoT model to perform text-to-image generation with self-check (self-reflection) reasoning mechanism.</p>
  <p>&#x1F525; We are still working on the this project to implement more kinds of Chain-of-Thought (CoT) mechanism into unified model, please stay tuned!</p>
</div>

<div class="content">
  <h2 style="text-align:center;">Overview</h2>
  <img src="./files/pipeline.png" alt="Uni-CoT Pipeline" class="teaser-gif" style="width:100%;">

  <p>Chain-of-Thought (CoT) reasoning has significantly enhanced LLM performance on complex text tasks by encouraging interpretable, step-by-step problem solving. However, extending this paradigm to multimodal tasks presents new challenges. In vision-language scenarios, human cognition depends on understanding how visual states evolve over time, inferring causality and planning based on object movements, spatial interactions, and transformations, which are critical for physical reasoning, visual planning, and story comprehension.</p>
  <p>To bridge this gap, we introduce the Unified Chain-of-Thought (Uni-CoT) framework, designed to empower Multimodal Large Language Models (MLLMs) to perform structured and interpretable reasoning across both text and vision. Uni-CoT first decomposes a given multimodal task into simple, modular steps, and then processes each step either sequentially or in parallel, as illustrated below. Thus, it enables more systematic and scalable reasoning across modalities.</p>
  <p>Specifically, the Uni-CoT reasoning pipeline consists of four key components:</p>
  <p>&nbsp;&nbsp;&nbsp;1. <b>Planning</b>: Decompose the complex task into a sequence of simpler, manageable subtasks.</p>
  <p>&nbsp;&nbsp;&nbsp;2. <b>Subtask Execution</b>: Execute each subtask using the unified model with step-by-step reasoning.</p>
  <p>&nbsp;&nbsp;&nbsp;3. <b>Self-Check</b>: After completing each subtask, perform a validation check to ensure the intermediate result aligns with the intended sub-goal.</p>
  <p>&nbsp;&nbsp;&nbsp;4. <b>Final Result</b>: Aggregate the validated subtask results to generate the final output.</p>
  <p>With these designs, our Uni-CoT framework aims to enable unified large models to tackle a wide range of challenging multimodal applications, including: </p>
  <p>&nbsp;&nbsp;&nbsp;1. <b>Highly reliable image generation/editing; </b></p>
  <p>&nbsp;&nbsp;&nbsp;2. <b>Visual planning; </b></p>
  <p>&nbsp;&nbsp;&nbsp;3. <b>Geometric and physical reasoning.</b></p>
</div>

<div class="content">
  <h2 style="text-align:center;">Results</h2>
  With our Uni-CoT design, currently we have achieve reliable image generation across various scenarios, moreover achieving sota performance on <a href="https://github.com/Fr0zenCrane/UniCoT" target="_blank">[WISE]</a> benchmark.
  <h3>Qualitative Results</h3>
  <img src="./files/qualitative_results.png" alt="Qualitative Results" class="teaser-gif" style="width:75%;">
  <h3>Quantitative Results</h3>
  <h3 style="text-align:center;">WISE</h3>
  <table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; width: 100%; font-family: Arial, sans-serif; margin: 20px 0;">
    <thead>
      <tr>
        <th></th>
        <th>Culture↑</th>
        <th>Time↑</th>
        <th>Space↑</th>
        <th>Biology↑</th>
        <th>Physics↑</th>
        <th>Chemistry↑</th>
        <th>Overall↑</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Janus</td>
        <td>0.16</td>
        <td>0.26</td>
        <td>0.35</td>
        <td>0.28</td>
        <td>0.30</td>
        <td>0.14</td>
        <td>0.23</td>
      </tr>
      <tr>
        <td>MetaQuery</td>
        <td>0.56</td>
        <td>0.55</td>
        <td>0.62</td>
        <td>0.49</td>
        <td>0.63</td>
        <td>0.41</td>
        <td>0.55</td>
      </tr>
      <tr>
        <td>Bagel</td>
        <td><b>0.76</b></td>
        <td><b>0.69</b></td>
        <td><u>0.75</u></td>
        <td><u>0.65</u></td>
        <td><u>0.75</u></td>
        <td><u>0.58</u></td>
        <td><u>0.70</u></td>
      </tr>
      <tr>
        <td><b>Uni-CoT</b></td>
        <td><u>0.75</u></td>
        <td><u>0.66</u></td>
        <td><b>0.78</b></td>
        <td><b>0.70</b></td>
        <td><b>0.78</b></td>
        <td><b>0.71</b></td>
        <td><b>0.73</b></td>
      </tr>
      <tr>
        <td><i>GPT4O</i></td>
        <td><i>0.81</i></td>
        <td><i>0.71</i></td>
        <td><i>0.89</i></td>
        <td><i>0.83</i></td>
        <td><i>0.79</i></td>
        <td><i>0.74</i></td>
        <td><i>0.80</i></td>
      </tr>
    </tbody>
  </table>
</div>

<div class="content">
  <h2 style="text-align:center;">Method</h2>
  <h3>Key Observation</h3>
  <p>We adapt the unified Bagel-7B-MoT model to perform joint text and image generation in support of UniCoT-style multimodal reasoning. As a first step, we fine-tune the model using its native interleaved text-image training paradigm. While this naïve adaptation enables the model to learn basic UniCoT behaviors, we observe significant challenges when scaling to complex reasoning chains involving multiple image-text steps. </p>
  <p>A primary bottleneck lies in the elevated complexity introduced by visual reasoning. Unlike text-only reasoning, where each step typically consumes 512–1,024 tokens, UniCoT requires generating both a reasoning text and a corresponding image per step. Synthesising Image via VAE-based representation consumes ~4,096 tokens, and encoding the image with a ViT-based representation for understanding incurs an additional ~4,900 tokens, resulting in nearly 9,000 tokens per reasoning step. This substantial overhead significantly increases the computational cost of training and inference. As the reasoning chain grows, the model struggles to converge and generalize, ultimately limiting its performance on complex multimodal tasks.</p>
  <img src="./files/motivation.png" alt="Motivation" class="teaser-gif" style="width:75%;">
  <h3>Our Solution</h3>
  <p>To mitigate the complexity introduced by long multimodal reasoning chains, we reformulate the Uni-CoT process as a Markov Decision Process (MDP), where each step depends solely on the current state. Concretely, we model each reasoning step as a discrete MDP node, which only depends on the preceding step and the task instruction. This formulation enables the model to focus on learning local transition dynamics between adjacent nodes, rather than capturing dependencies across the entire reasoning chain as shown below. Such a design choice significantly reduces computational overhead and improves training efficiency.</p>
  <img src="./files/mdp_process.png" alt="MDP Process" class="teaser-gif" style="width:60%;">
  <p>Specifically, each MDP node is defined by the following components:</p>
  <p><b>&nbsp;&nbsp;&nbsp;State \(s_t\)</b>: Current context, refer to last reasoning step, including both text and images.</p>
  <p><b>&nbsp;&nbsp;&nbsp;Action \(a_t\)</b>: A hybrid operation that involves generating editing instructions and performing corresponding image edits.</p>
  <p><b>&nbsp;&nbsp;&nbsp;Next State \(s_{t+1}\)</b>: The updated context resulting from the applied action, including the edited image, a textual summary according to the edited image.</p>
  <p><b>&nbsp;&nbsp;&nbsp;Reward \(r_t\)</b>: A textual conclusion or scalar score that quantifies the alignment between the outcome and the task objective.</p>
  <figure style="text-align: center;">
    <img src="./files/mdp_architecture.png" alt="MDP Architecture" class="teaser-gif" style="width:60%;">
    <figcaption>Uni-CoT components that requires loss during training are highlighted in pink.</figcaption>
  </figure>
  <p>With above design, our training focuses on three core objectives:</p>
  <p>&nbsp;&nbsp;&nbsp;1. Learning to generate <b>hybrid actions</b> (text and image edits) that drive reasoning progression.</p>
  <p>&nbsp;&nbsp;&nbsp;2. Predicting the <b>next state summary</b> given the current state and action.</p>
  <p>&nbsp;&nbsp;&nbsp;3. Estimating <b>reward</b> that reflect task completion and reasoning quality.</p>
</div>


<div class="content">
  <h2 style="margin-top: 0; color: #333;">BibTeX</h2>
  <pre style="white-space: pre-wrap; word-break: break-word; font-size: 0.95em; color: #2c3e50;">
@misc{Uni-CoT,
  author       = {SAIS-FUXI},
  title        = {Uni-CoT: Towards Unified Chain-of-Thought Reasoning Across Text and Vision},
  howpublished = {\url{https://github.com/Fr0zenCrane/UniCoT}},
  year         = {2025},
  note         = {Accessed: 2025-07-28}
}
  </pre>
</div>

<div class="content">
  <h2 style="margin-bottom: 12px;">Acknowledgement</h2>
  <ul style="padding-left: 20px; margin: 0;">
    <li>
      We thank the <a href="https://github.com/ByteDance-Seed/Bagel" target="_blank"><strong>ByteDance-Seed</strong></a> team for proposing <strong>Bagel</strong>, a powerful and widely adopted unified model for multimodal understanding and generation. Bagel serves as a strong foundation for our work and enables the development of <strong>Uni-CoT</strong>.
    </li>
    <li>
      We also acknowledge the <a href="https://github.com/PKU-YuanGroup/WISE" target="_blank"><strong>PKU-YuanGroup</strong></a> for introducing <strong>WISE</strong>, a comprehensive benchmark for evaluating text-to-image models on complex semantic understanding and world knowledge integration. Its emphasis on reasoning makes it an ideal testbed for CoT-style self-check in multimodal contexts.
    </li>
    <li>
      The project page design is adapted from <a href="https://dreambooth.github.io/" target="_blank"><strong>DreamBooth</strong></a>, which we gratefully credit for its elegant and effective presentation.
    </li>
  </ul>
</div>
</body>
</html>
