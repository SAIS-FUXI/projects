<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Uni-CoT: Towards Unified Chain-of-Thought Reasoning Across Text and Vision</title>
<link href="./files/style.css" rel="stylesheet">
<script type="text/javascript" src="./files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./files/jquery.js"></script>
</head>
<style> .centered {text-align: center;}</style>

	
<body>
<div class="content">
  <h1><strong>Uni-CoT: Towards Unified Chain-of-Thought Reasoning Across Text and Vision</strong></h1>
  <p id="authors" class="serif">
    <span style="font-size: 0.9em">
    <a>Luozheng Qin<sup>1*</sup></a>
    <a>Jia Gong<sup>1*</sup></a>
    <a>Yuqing Sun<sup>1*</sup></a>
    <a>Tianjiao Li<sup>3</sup></a>
    <a>Mengping Yang<sup>1</sup></a>
    <a>Xiaomeng Yang<sup>1</sup></a>
    <a>Zhiyu Tan<sup>1,2#</sup></a>
    <a>Hao Li<sup>1,2&dagger;</sup></a>
    </span>
    <br>
  <span style="font-size: 1.0em; margin-top: 0.6em">
    <a><sup>1</sup>ShangHai Academy of AI for Science</a>
    <a><sup>2</sup>Fudan University</a>
    <a><sup>2</sup>Nanyang Technological University</a>
  <br>
  </span>
<span style="font-size: 12pt;"><b><sup>*</sup>Equal contribution</b><b><sup>#</sup>Project leader</b><b><sup>&dagger;</sup>Corresponding author</b></span>
  </p>
    <font size="+2">
          <p style="text-align: center;">
      <a href="https://sais-fuxi.github.io/projects/uni-cot" target="_blank">[Page]</a> &nbsp;&nbsp;&nbsp;&nbsp;
      <a href="https://github.com/Fr0zenCrane/UniCoT" target="_blank">[Code]</a> &nbsp;&nbsp;&nbsp;&nbsp;
      <a href="https://huggingface.co/Fr0zencr4nE/UniCoT-7B-MoT" target="_blank">[Model]</a> &nbsp;&nbsp;&nbsp;&nbsp;
      <a href="files/bibtex.txt" target="_blank">[BibTeX]</a>
          </p>
    </font>
</div>
<div class="content">
  <h2 style="text-align:center;">&#x1F525;News</h2>
  <p>&#x2705; 2025.07.29 We released UniCoT-7B-MoT, which extends Bagel-7B-MoT model to perform text-to-image generation witn self-reflection reasoning mechanism.</p>
  <p>&#x1F525; We are still working on the this project to implement more kinds of Chain-of-Thought (CoT) mechanism into unified model, please stay tuned!</p>
</div>

<div class="content">
  <h2 style="text-align:center;">Overview</h2>
  <img src="./files/pipeline.png" class="teaser-gif" style="width:75%;">

  <p>Chain-of-Thought (CoT) reasoning has significantly enhanced LLM performance on complex text tasks by encouraging interpretable, step-by-step problem solving. However, extending this paradigm to multimodal tasks presents new challenges. In vision-language scenarios, human cognition depends on understanding how visual states evolve over time, inferring causality and planning based on object movements, spatial interactions, and transformations, which are critical for physical reasoning, visual planning, and story comprehension.</p>
  <p>To bridge this gap, we introduce the Unified Chain-of-Thought (Uni-CoT) framework, designed to empower Multimodal Large Language Models (MLLMs) to perform structured and interpretable reasoning across both text and vision. Uni-CoT first decomposes a given multimodal task into simple, modular steps, and then processes each step either sequentially or in parallel, as illustrated below. Thus, it enables more systematic and scalable reasoning across modalities.</p>
  <p>Specifically, the Uni-CoT reasoning pipeline consists of four key components:</p>
  <p>1. <b>Planning</b>: Decompose the complex task into a sequence of simpler, manageable subtasks.</p>
  <p>2. <b>Subtask Execution</b>: Execute each subtask using the unified model with step-by-step reasoning.</p>
  <p>3. <b>Self-Check</b>: After completing each subtask, perform a validation check to ensure the intermediate result aligns with the intended goal.</p>
  <p>4. <b>Final Result</b>: Aggregate the validated subtask results to generate the final output.</p>
  <p>With these designs, our Uni-CoT framework aims to enable unified large models to tackle a wide range of challenging multimodal applications, including: </p>
  <p>Highly reliable image generation/editing</p>
  <p>Visual planning</p>
  <p>Geometric and physical reasoning</p>
</div>
<div class="content">
  <h2>Human-aligned Detailed Video Caption Scorer</h2>
  <img src="./files/scorer_pipeline.png" class="teaser-gif" style="width:75%;">
  <p>As illustrated in the above figure. The core of Cockatiel captioner relies on a human-aligned caption scorer, which assess the training value of each candidate synthesized caption from the perspective of dimension-specific video-caption alignment and human preference. In this way, we can avoid the impairment introduced by the synthetic nature of our data and align them with human preferences, eventually bootstrapping model performance on VDC and encouraging the generation of human-preferred captions. However, to the best of our knowledge, no public model nor training dataset currently suits this need, so we have to build it on our own. Specifically, we meticulously annotate a dataset of structured human preference score on video detailed captions and fine-tune VILA-v1.5-13B on it.</p>
</div>
<div class="content">
  <h2>Cockatiel Captioner</h2>
  <img src="./files/pipeline.png" class="teaser-gif" style="width:75%;"><br>
  <p>To infuse VDC models with captioning knowledge on every fine-grained dimension of video-caption alignment and human preferences, we devise a three-stage training pipeline to implement the proposed ensembling synthetic and human preferenced training while meet common engineering need. Specifically, we curate data employing the scorer-based selection policy with threshold, which assess the training value of captions generated by three base models, LLaVA-Video-7B, VILA-v1.5-13B, Aria3.5Bx8. Moreover, it scores each candidate caption and exclusively involve the one with the highest score for training if it exceeds the preset threshold. With the abovementioned synthetic data reject sampling procedure, we then train our Cockatiel-13B captioner based on the data, and further distill Cockatiel-8B from Cockatiel-13B. 
</p>

</div>
<div class="content">
  <h2>Results</h2>
  <p>We provide some specific comparison cases between Cockatiel-13B and leading VDC models in the below figure. For more detailed comparisons or more quantitative and qualitative results, please refer to our paper </p>
<img src="./files/qualitative.png" class="teaser-gif" style="width:75%;">
</div>
<div class="content">
  <h2>BibTex</h2>
  <code> @misc{qin2025cockatielensemblingsynthetichuman,<br>
  &nbsp;&nbsp;title={Cockatiel: Ensembling Synthetic and Human Preferenced Training for Detailed Video Caption},<br>
  &nbsp;&nbsp; author={Luozheng Qin and Zhiyu Tan and Mengping Yang and Xiaomeng Yang and Hao Li},<br>
  &nbsp;&nbsp;year={2025},<br>
  &nbsp;&nbsp;eprint={2503.09279},<br>
  &nbsp;&nbsp;archivePrefix={arXiv},<br>
  &nbsp;&nbsp;primaryClass={cs.CV},<br>
  &nbsp;&nbsp;url={https://arxiv.org/abs/2503.09279}, <br>
  } </code> 
</div>
<div class="content">
  <h2>Acknowledgement</h2>
  The project page template is borrowed from <a href="https://dreambooth.github.io/">DreamBooth</a>.
</div>
</body>
</html>
